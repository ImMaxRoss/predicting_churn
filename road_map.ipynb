{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification project → Categorical target\n",
    "###    a. Explore target, how many classes (binary or multiclass)?\n",
    "### b. Is there a class imbalance? If so might need to consider certain techniques\n",
    "        i. Either SMOTE or class_weight maybe\n",
    "###    c. Determine the business context/problem for stakeholder\n",
    "        i. How is your model helping to address/solve the problem?\n",
    "###    d. Will you want some form of inference (will affect model choices)\n",
    "       i. Can be very beneficial\n",
    "###    e. Determine which evaulation metric/metrics are most important\n",
    "        i.THIS IS BIG\n",
    "        ii. Think in terms of cost matrix, fp vs fn etc…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{bmatrix}\n",
    "TN & FP \\\\\n",
    "FN & TP\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tn = customer predicted no churn and it's true\n",
    "fp = customer predicted churn and it's false\n",
    "fn = customer predicted no churn and it's false\n",
    "tp = customer predicted churn and it's true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Binary\n",
    "b. Yes, 86% non-churned, 14% churned\n",
    "    i. going to use SMOTE\n",
    "c. Model is going to accurately predict customers that will churn in order to save money by not wasting campaign dollars on customers that would not have churned.\n",
    "d. Yes, I will want to use inferential statistics to determine the biggest factors effecting churn\n",
    "e. The most important metric is false positives. We want to limit our false positives so our most important metric is going to be **Prescision** and **Recall**, **F1-Score** will also be using **AUC-ROC** since there is substantial imbalance and the value of false positives and false negatives is significantly different. Also will be using Confusion Matrix to show the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA → Explore your dataset and features\n",
    "    a. Basic summary info, .describe, null or missing values, column types\n",
    "    b. What features are relevant for your model/analysis and why? \n",
    "    c. Relationships with target\n",
    "    d. Exploratory visuals, correlations, pairplots, heatmaps, histograms etc…\n",
    "    e. Determine how you will handle null values\n",
    "        i. Impute values, drop rows or columns etc…\n",
    "        ii. Take a look at the different types of imputers (KNNI is powerful but slow)\n",
    "    f. Determine how you will handle categorical variables\n",
    "        i. Binary?\n",
    "        ii. Ordinal or One-Hot encode?\n",
    "    g. Will you need to scale the numeric data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. No null values, and no duplicates\n",
    "b. all features will be used for the model besides:\n",
    "    i. 'phone number' (unique for every instance and therefore not significant)\n",
    "    ii. 'area code' & 'state' (does not indicate usage of their service)\n",
    "    iii. 'total charge' has high correlation with '..total minutes' and would cause high multi-colliniearity and total minutes is preferable since it is more indicative of a users usage.\n",
    "    iv. dropping number of voicemail messages since it would cause multi collinearity and voicemail plan has a higher correlation with churn\n",
    "c. All int/float columns measure usage of a customer. Category columns define different plans for the user\n",
    "d. COME BACK TO THIS LATEER\n",
    "e. no null values\n",
    "f. handling categorical\n",
    "    i. categorical columns are binary and will be converted to boolean int (0 & 1)\n",
    "g. Will need to scale the numeric data since there is several different units being measured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Determine an appropriate validation procedure\n",
    "    a. Highly recommended to cross_validate with a pipeline (best way)\n",
    "    b. train_test_split your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['churn', 'phone number', 'area code'], axis=1)\n",
    "y = df['churn']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Develop ColumnTransformer → data preprocessing pipeline based on EDA above\n",
    "    a. Sub-pipes for numeric and categorical columns\n",
    "        i. As many as you need if treating some columns differently\n",
    "        ii. Keep in mind things like handle_unknown = ‘ignore’\n",
    "    b. Create one pipeline with numeric scaling, another without if needed\n",
    "    c. Test your column transformer\n",
    "        i. Fit_transform train data\n",
    "        ii. Transform test data\n",
    "        iii. Should have the same number of columns after transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpipe_num = Pipeline(steps=[('ss', StandardScaler())])\n",
    "\n",
    "\n",
    "\n",
    "subpipe_cat = Pipeline(steps=[('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))])\n",
    "\n",
    "\n",
    "\n",
    "num_cols = ['account length','number vmail messages', 'total day minutes', 'total day calls',\n",
    "           'total day charge', 'total eve minutes', 'total eve calls', 'total eve charge',\n",
    "           'total night minutes', 'total night calls', 'total night charge', 'total intl minutes',\n",
    "           'total intl calls', 'total intl charge', 'customer service calls']\n",
    "\n",
    "\n",
    "\n",
    "cat_cols = ['international plan', 'voice mail plan', 'state']\n",
    "\n",
    "\n",
    "\n",
    "CT = ColumnTransformer(transformers=[('subpipe_num', subpipe_num, num_cols),\n",
    "                                     ('subpipe_cat', subpipe_cat, cat_cols)],\n",
    "                       remainder='passthrough')\n",
    "\n",
    "\n",
    "\n",
    "CT.fit_transform(X_train).shape, CT.fit_transform(X_test).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modelling Process → Pipeline using ColumnTransformer and model algorithm\n",
    "    a. Start with a DummyClassifier\n",
    "        i. Evaluate based on chosen metric/metrics → need to beat this\n",
    "    b. Create and evaluate first simple model: Simple → Complex\n",
    "        i. Start with defaults, logistic regression or decision tree based on your data\n",
    "        ii. Could try other algorithms if you think warranted, KNN, Naive Bayes\n",
    "    c. Iterate over previous models\n",
    "        i. Use GridSearch to tune hyperparameters\n",
    "            1a. Remember you can tweak parts of the CT as well\n",
    "        ii. If overfit → reduce complexity, add regularization, prune tree\n",
    "        iii. If underfit → increase complexity, reduce regularization, add new/more features (feature engineering)\n",
    "        iv. Might require you to go back and adapt the pre-processing pipeline\n",
    "    d. Consider using an ensemble model for more complexity and predictive power\n",
    "        i. RandomForest, ExtraTrees, VotingClassifier, StackingClassifier etc…\n",
    "        ii. Tune these via GridSearch\n",
    "        iii. Iterate your heart out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##b. If overfit → reduce complexity, add regularization, prune tree\n",
    "##c. If underfit → increase complexity, reduce regularization, add new/more features (feature engineering)\n",
    "##d. Might require you to go back and adapt the pre-processing pipeline\n",
    "\n",
    "#2. Consider using an ensemble model for more complexity and predictive power\n",
    "##a. RandomForest, ExtraTrees, VotingClassifier, StackingClassifier etc…\n",
    "##b. Tune these via GridSearch\n",
    "##c. Iterate your heart out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Start with a DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy='stratified', random_state=42)\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "print(precision_score(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Create and evaluate first simple model: Simple → Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_pipe = ImPipeline(steps=[('ct', CT),\n",
    "                             ('sm', SMOTE(random_state=42)),\n",
    "                            ('dectree', DecisionTreeClassifier(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = imb_pipe.predict(X_test)\n",
    "print(precision_score(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Iterate over previous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    'dectree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {'criterion': ['gini', 'entropy'], 'max_depth': [None, 10, 20, 30]}\n",
    "    }\n",
    "}\n",
    "\n",
    "for model_name, model_params in models.items():\n",
    "    model = model_params['model']\n",
    "    params = model_params['params']\n",
    "    \n",
    "    clf = GridSearchCV(model, params, cv=5, n_jobs=-1)\n",
    "    clf.fit(X_train_ct, y_train)\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {clf.best_params_}\")\n",
    "    print(f\"Accuracy score of {model_name}: {clf.score(X_test_ct, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Final Model → Thursday AM you should have a good idea here\n",
    "    a. Choose a final model based on validation scores of the chosen metric/metrics\n",
    "    b. Fit final model to training set\n",
    "    c. Evaluate the final model using your hold-out test set\n",
    "    d. Discuss final model in the context of your business problem and stakeholder\n",
    "        i. Analyze your predictive power and results, where is the model doing good and where is it maybe not doing good (confusion matrix)\n",
    "        ii. This could include insights and recommendations from model\n",
    "            iia. Coefs (logreg)\n",
    "            iib. Feature importances (trees, rf, boosting etc…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Explanatory visuals for presentation\n",
    "    a. Back up any inference with visual\n",
    "    b. Show final model vs. others (esp. Dummy)\n",
    "    c. Spiced up confusion matrix, need to make sure its non-technical enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
